{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb5e34a9-90de-465a-97d5-b216d571d8ca",
   "metadata": {},
   "source": [
    "# Final Project Functions List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cad9d49-89f8-4c73-a118-76e997ab8395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5507329-30cc-46e8-9649-a661083fdf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_threshold(y_predict, y_prob, y_test):\n",
    "    \"\"\"\n",
    "    Returns plot of precision / recall vs threshold\n",
    "    \n",
    "        Parameters:\n",
    "            y_predict(float):  y_predict from fitted model\n",
    "            y_prob(float):     y_probability from fitted model (predict_proba)\n",
    "            y_test(array):     target values from test set\n",
    "            \n",
    "        Output:\n",
    "            Plot      \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, probs_y[:, 1]) \n",
    "\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "    plt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\n",
    "    plt.ylabel(\"Precision, Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.ylim([0,1])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "    \n",
    "def top_categories(X, perc):\n",
    "    \"\"\"\n",
    "    Returns dominant groups in category by %\n",
    "    \n",
    "        Parameters:\n",
    "            X (list):      column of values\n",
    "            perc (float):  threshold for cutoff (percentage)\n",
    "            \n",
    "        Returns:\n",
    "            List of top groups\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    val_list = []\n",
    "    key_list = []\n",
    "    denom = sum(X.values())\n",
    "    \n",
    "    for count, (keys, values) in enumerate(X.items()):\n",
    "        if (sum(val_list)/denom) < perc:\n",
    "            key_list.append(keys)\n",
    "            val_list.append(values)\n",
    "            \n",
    "    return val_list, key_list\n",
    "\n",
    "\n",
    "def clf_metrics(model,X_test,y_test):\n",
    "    \"\"\"\n",
    "    Returns various classification evaluation metrics\n",
    "    \n",
    "        Parameters:\n",
    "            model ():         ML model to be evaluated\n",
    "            X_test (df):      scaled test data used to evaluate the model\n",
    "            y_test (series):  target data used for evaluating predictions\n",
    "        \n",
    "        Returns:\n",
    "            Classification report and plots of roc curve and confusion matrix\n",
    "            \n",
    "    \n",
    "    \"\"\"\n",
    "    clf_rep = classification_report(y_test, (model.predict(X_test)))\n",
    "    roc = plot_roc_curve(model, X_test, y_test)\n",
    "    mtrx = plot_confusion_matrix(model, X_test, y_test)\n",
    "    \n",
    "    \n",
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    Returns two graphs:\n",
    "        Training and validation accuracy\n",
    "        Traning and validation loss\n",
    "        \n",
    "    Parameters:\n",
    "        Trained keras model.fit\n",
    "    \"\"\"\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bb6b09-bd7a-4216-b979-66e5525b985d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LighthouseLabs",
   "language": "python",
   "name": "lighthouselabs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
